\documentclass[11pt]{article} % use larger type; default would be 10pt
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

\input{header_stuff.tex}


%%% END Article customizations

%%% The "real" document content comes below...

{\title{Reimplementing Prefix-Tuning: Optimizing Continuous Prompts for Generation}}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 
\author{Mac Turner, Michael Ngo, Eric Hu, Neeraj Parihar}


\begin{document}
\maketitle
\begin{center}
    \vspace{-2em}
    \href{https://github.com/Mikonooooo/prefix-tuning}{github.com/Mikonooooo/prefix-tuning}
\end{center}

\section{Introduction}
Finetuning LLMs for specific tasks requires training a new set of weights for every task, which is expressive but expensive. On the other hand, prompt engineering does not require any training, which is cheap but limiting. \textit{Prefix-Tuning: Optimizing Continuous Prompts for Generation} by Xiang Lisa Li and Percy Liang~\cite{li-liang-2021-prefix} introduces a method of fine-tuning that combines the efficiency of prompt engineering and the effectiveness of full fine tuning.

The paper shows that prefix tuning GPT-2, BART and T5 on table-to-text generation and text summarization performs better than fine tuning and other parameter efficient fine tuning methods. Additionally, prefix tuning is more parameter efficient and thus faster to train than other fine tuning methods.

\section{Chosen Result}

We chose to reproduce prefix-tuning for GPT-2 Medium on table-to-text generation and compare it against full fine tuning. This is the first two rows and the first column of Table 1 in the original paper (and the last two rows of the Table 1 in this report). We chose this result because it shows the major result of the paper: that prefix-tuning is comparable, if not better than fine tuning. We chose to implement prefix-tuning only with GPT-2 and the E2E table-to-text dataset, since using other models and datasets would just be reimplementing the same method using different code infrastructures. 


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{table1.png}
%     \caption{Table 1 from the original paper}
%     \label{fig:enter-label}
% \end{figure}

\section{Methodology}

\subsection{Prefix-Tuning Architecture}
See Figure~\ref{fig:prefix_tuning}. The base model we freeze is GPT-2 Medium. The model is a stack of attention blocks with hidden size $768$. For each attention block $i$, Prefix-Tuning learns $L$ keys and values to attended over during generation. These $L$ keys and values are learned via a network, $\textsf{MLP}_{\theta,i}$ and input matrix $P_{\theta,i}$. This is done for training stability. $P_{\theta,i}$ is of dimension $L\times 768$. The MLP is 2 layers, specifically a linear layer from $768$ to $800$, followed by a $\textsf{tanh}$ activation, and a linear layer from $800$ to the embedding dimension of $2\cdot 784$. Note there is multiplication by $2$ because for each activation, we need a key and value.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{prefix-architecture.png}
    \caption{Prefix-Tuning Architecture}
    \label{fig:prefix_tuning}
\end{figure}

To do prefix-tuning, we load a pretrained GPT-2 Medium model from HuggingFace and pass in the learned keys and values through the \verb|past_key_values| keyword. Additionally, generation was done via beam search with beam length of $5$.

\subsection{Dataset}
The dataset is the E2E Table-to-text generation dataset~\cite{novikova-etal-2017-e2e}. It is a dataset of tables conaining information about restaurants and the goal is to write a sentence that summaries the tabular information. Generated or proposed sentences are compared to a list acceptable reference sentences and evaluated with a standard suite of metrics: BLEU, NIST, METEOR, ROUGE-L, and CIDEr.

\subsection{Training}
Following the paper, prefix-tuning is trained over the E2E dataset, with \texttt{epochs=5}, \texttt{lr=8e-5}, \texttt{batch\_size=10}, \texttt{prefix\_length=5}. Full GPT fine-tuning uses \texttt{epochs=5}, \texttt{lr=5e-5}, \texttt{batch\_size=10}.

\section{Results \& Analysis}

\begin{table}[H]
    \centering
    \begin{tabular}{c|ccccc}
        \textbf{Model} & \textbf{BLEU} & \textbf{NIST} & \textbf{METEOR} & \textbf{ROUGE-L} & \textbf{CIDEr} \\
        \hline
        Prefix-Tuning (0.1\%) (our impl.) & \textbf{68.8} & \textbf{8.80} & 45.7 & \textbf{71.3} & 2.41 \\
        Fine-Tuning (our impl.) & \textbf{68.8} & 8.68 & \textbf{45.8} & 71.2 & \textbf{2.45}\\
        \hline
        Prefix-Tuning (0.1\%)~\cite{li-liang-2021-prefix}  & \textbf{69.7} & \textbf{8.81} & 46.1 & \textbf{71.4} & \textbf{2.49} \\
        Fine-Tuning~\cite{li-liang-2021-prefix} & 68.2 & 8.62 & \textbf{46.2} & 71.0 & 2.47
    \end{tabular}
    \caption{Results of prefix-tuning and full fine-tuning of GPT-2 Medium on the E2E table-to-text generation dataset.}
    \label{tab:results}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{image.png}
    \caption{BLEU Score vs Prefix Length For E2E Table-to-Text Generation Task}
    \label{pref_len}
\end{figure}

See results in Table~\ref{tab:results}. We observe that our results are very slightly worse than the paper's reported scores. However, the general trends we observed are the same: prefix-tuning has comparable performance to fine-tuning GPT-2, while optimizing much less (0.1\%) parameters. We also read through the model outputs, and could not discern between prefix-tuned and fine-tuned GPT-2, as their outputs were both reasonable. We also performed an ablation on the prefix length, shown in Figure~\ref{pref_len}, where we found that performance stagnates with prefixes with length $>5$. This is consistent with the authors' choice to use prefixes of length 5 for this task. Finally, we benchmarked the time taken for each type of tuning. The paper reported a ratio of \texttt{time to fine-tune / time to prefix-tune} of $\approx$ 1.5, and our calculated ratio of $\approx$ 1.8. 

We faced a lot of challenges during implementation. Using the HuggingFace transformer's \texttt{generate} function produced results that omitted table information. After a lot of debugging, we decided to implement it ourselves, which includes implementing beam search from scratch. We also spent a good chunk of time figuring out how to properly pass in prefix information to the HuggingFace model.

Despite these challenges, our results align with those found the original authors: prefix tuning is about as effective as finetuning while being much more time and space efficient.

\section{Reflections}
We learned to iterate early and often. While understanding concepts is useful to map out potential roadblocks, it arguably more beneficial to write up code and see what actually becomes a roadblock. Prefix tuning still requires new prefixes to be trained for each task. We thought it would be interesting to have an LLM output specific tokens to utilize specific prefix weights when it needs to perform a certain task.

\bibliography{ref}



\end{document}